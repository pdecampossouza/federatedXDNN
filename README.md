# ğŸ§  Federated xDNN for Interpretable Dental View Classification

This repository implements **Federated Explainable Deep Neural Networks (FxDNN)** â€” a framework that combines **federated learning** with **explainable prototype-based classification** for dental view image analysis.

The project proposes a privacy-preserving and interpretable model that allows multiple clinical sites to collaboratively train a shared encoder while keeping their local data private. Each client trains its own **xDNN classifier** locally on embeddings extracted from a global encoder. Interpretability is achieved through **prototype inspection** and **SHAP-based visual explanations**.

---

## ğŸš€ Key Features

- **Federated Encoder Training** â€” Train a shared image encoder (`ResNet18` or `ResNet50`) across distributed clients using **FedAvg** or **FedProx**.  
- **Local xDNN Classifiers** â€” Each client learns an **xDNN** model based on **prototypes** extracted from its embeddings, without sharing data.  
- **Explainability via SHAP** â€” Generates **heatmaps** and **prototype visualizations** that highlight image regions contributing to classification.  
- **Privacy by Design** â€” No raw data or prototypes are exchanged â€” only model weights are aggregated.

---

## ğŸ§© Repository Structure

ProtoPNet+xDNN/
â”‚
â”œâ”€â”€ fed_xdnn.py # Main federated and local training script
â”œâ”€â”€ analyze_prototypes_shap.py # SHAP-based visual interpretation of prototypes
â”‚
â”œâ”€â”€ encoder_global.pt # Federated global encoder (saved model)
â”œâ”€â”€ siteA_xdnn.pkl # Local xDNN model for client A
â”œâ”€â”€ siteB_xdnn.pkl # Local xDNN model for client B
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ clients/
â”‚ â”œâ”€â”€ siteA/
â”‚ â”‚ â”œâ”€â”€ train/{Frontal,Lateral,Oclusal}/...
â”‚ â”‚ â””â”€â”€ val/{Frontal,Lateral,Oclusal}/...
â”‚ â””â”€â”€ siteB/
â”‚ â”œâ”€â”€ train/{Frontal,Lateral,Oclusal}/...
â”‚ â””â”€â”€ val/{Frontal,Lateral,Oclusal}/...
â”‚
â”œâ”€â”€ shap_siteA/ # SHAP visual explanations for siteA
â””â”€â”€ shap_siteB/ # SHAP visual explanations for siteB

---

## âš™ï¸ Installation

```bash
git clone https://github.com/<your-username>/Federated-xDNN.git
cd Federated-xDNN
python -m venv .venv
# Windows: .venv\Scripts\activate
# macOS/Linux: source .venv/bin/activate
pip install -r requirements.txt
Required packages (if you donâ€™t use requirements.txt):

nginx
Copiar cÃ³digo
torch torchvision numpy scikit-learn scipy shap matplotlib opencv-python
ğŸ—ï¸ Federated Encoder Training
Train a global encoder collaboratively across all client datasets.

bash
Copiar cÃ³digo
python fed_xdnn.py --mode federated \
  --clients_root data/clients \
  --rounds 5 --local_epochs 1 --batch_size 32 --lr 3e-4 \
  --model resnet18 --image_size 224 --fedprox_mu 0.0 \
  --save_encoder encoder_global.pt
This performs FedAvg (or FedProx if --fedprox_mu > 0) to stabilize training with heterogeneous data.

ğŸ§  Local xDNN Training
Each site uses the global encoder to build its local explainable model:

bash
Copiar cÃ³digo
python fed_xdnn.py --mode local_xdnn \
  --clients_root data/clients --client_id siteA \
  --encoder_path encoder_global.pt \
  --batch_size 32 --image_size 224
Outputs:

Training/validation metrics (accuracy, precision, recall, F1, Îº)

Prototype counts per class

Saved model: siteA_xdnn.pkl (or siteB_xdnn.pkl)

ğŸ” SHAP-Based Prototype Explanation
Generate interpretability heatmaps for prototypes and their most similar images.

bash
Copiar cÃ³digo
python analyze_prototypes_shap.py \
  --clients_root data/clients --client_id siteA \
  --encoder_path encoder_global.pt --xdnn_path siteA_xdnn.pkl \
  --split val --per_class 1 --top_per_proto 1 --image_size 224 \
  --out_dir shap_siteA --alpha 0.35 --sigma 0.8 --cmap seismic
Output
Heatmaps overlaid on denormalized images (red = positive influence, blue = negative), illustrating how xDNN prototypes align with image features (e.g., occlusal line, lateral curvature, frontal symmetry).

ğŸ§® Evaluation Metrics
Metric	Description
Accuracy	Overall correctness
Precision / Recall / F1	Class-level performance
Cohenâ€™s Îº	Chance-corrected agreement
Confusion Matrix	Prediction distribution per class

ğŸ“ˆ Example Results
Client	Accuracy	F1	Îº	Notes
Site A	0.89	0.88	0.82	Minor confusion Frontal â†” Lateral
Site B	1.00	1.00	1.00	Fully converged
Global	â€“	â€“	â€“	Shared encoder generalized well

(Values from a representative run; results vary by seed and preprocessing.)

ğŸ§© Explainability Insight
Red regions: evidence for the predicted class.

Blue regions: evidence against the predicted class.

Prototype visualization confirms that xDNN focuses on semantically meaningful patterns.

ğŸ“š Citation
If you use this code or findings, please cite:

bibtex
Copiar cÃ³digo
@article{Souza2025FederatedxDNN,
  title     = {Federated xDNN for Interpretable Dental View Classification},
  author    = {Paulo Vitor de Campos Souza},
  journal   = {IEEE Journal of Translational Engineering in Health and Medicine},
  year      = {2025},
  note      = {Under Review}
}
ğŸ™ Acknowledgment
This work was supported by national funds through the FundaÃ§Ã£o para a CiÃªncia e a Tecnologia (FCT), under project UIDB/04152 â€“ Centro de InvestigaÃ§Ã£o em GestÃ£o de InformaÃ§Ã£o (MagIC), NOVA Information Management School (NOVA IMS), Universidade Nova de Lisboa, Portugal.

We also acknowledge the Brazilian Ministry of Health for providing access to calibration materials and public documentation from the SB Brasil 2023 National Oral Health Survey.

âš–ï¸ License
Released under the MIT License. You are free to use, modify, and distribute this code with proper citation.

ğŸ“¬ Contact
Paulo Vitor de Campos Souza
NOVA Information Management School (NOVA IMS)
Email: psouza@novaims.unl.pt

âœ³ï¸ â€œBridging public health and computer vision for interpretable oral-health AI.â€
